All Runs Results: lchaloupsky/czech-gpt2-oscar # Runs: 100 # Samples: 1000
Chars per token Poet data: 3.3737365770702397 +- 0.08508490747223248
Chars per token Base data: 4.26745283043716 +- 4.749154240660069e-15
All Runs Results: bigscience/bloom-560m # Runs: 100 # Samples: 100
Chars per token Poet data: 2.103622806600103 +- 0.05876504331474683
Chars per token Base data: 2.4303725088036376 +- 0.03490366294594056
All Runs Results: huggyllama/llama-7b # Runs: 100 # Samples: 100
Chars per token Poet data: 1.995397259297714 +- 0.057784291847850375
Chars per token Base data: 2.224823579528575 +- 0.033535667500591365
All Runs Results: lchaloupsky/czech-gpt2-oscar # Runs: 100 # Samples: 100
Chars per token Poet data: 3.378722000752648 +- 0.18046377959028967
Chars per token Base data: 4.272032104944646 +- 0.08819196457860504
All Runs Results: TheBloke/Llama-2-7B-fp16 # Runs: 100 # Samples: 100
Chars per token Poet data: 2.006011148571405 +- 0.06255614064204791
Chars per token Base data: 2.230111313573201 +- 0.03304025599165536
All Runs Results: huggyllama/llama-7b # Runs: 100 # Samples: 100
Chars per token Poet data: 1.9889094516089543 +- 0.05665709759544217
Chars per token Base data: 2.2357250859691775 +- 0.03118787784576438
All Runs Results: gpt2 # Runs: 100 # Samples: 100
Chars per token Poet data: 1.671656687866959 +- 0.04134872925774038
Chars per token Base data: 1.9874006437198843 +- 0.038271281560156065
All Runs Results: spital/gpt2-small-czech-cs # Runs: 100 # Samples: 100
Chars per token Poet data: 3.2280809705585143 +- 0.16293295954023107
Chars per token Base data: 3.7581932668570204 +- 0.08151683579482344
All Runs Results: C:\Users\micha\Desktop\Git\Tensorflow-Shorts\PoetGen\utils\tokenizers\BPE\tokenizer.json # Runs: 100 # Samples: 100
Chars per token Poet data: 4.043868203269542 +- 0.2028980151682649
Chars per token Base data: 3.773949323152452 +- 0.06814498830800811
All Runs Results: C:\Users\micha\Desktop\Git\Tensorflow-Shorts\PoetGen\utils\tokenizers\Unigram\tokenizer.json # Runs: 100 # Samples: 100
Chars per token Poet data: 3.612820671963123 +- 0.1293218132154293
Chars per token Base data: 2.8652366056491725 +- 0.056068972386957294
All Runs Results: C:\Users\micha\Desktop\Git\Tensorflow-Shorts\PoetGen\utils\tokenizers\WordPiece\tokenizer.json # Runs: 100 # Samples: 100
Chars per token Poet data: 3.887376361547298 +- 0.24634564379285215
Chars per token Base data: 9.737362298630016 +- 2.176732461956991
All Runs Results: C:\Users\micha\Desktop\Git\Tensorflow-Shorts\PoetGen\utils\tokenizers\BPE-Doubles\tokenizer.json # Runs: 100 # Samples: 100
Chars per token Poet data: 4.032971974862795 +- 0.16036852856198525
Chars per token Base data: 3.7700921513863523 +- 0.07307242630174511
All Runs Results: C:\Users\micha\Desktop\Git\Tensorflow-Shorts\PoetGen\utils\tokenizers\BPE\processed_tokenizer.json # Runs: 100 # Samples: 100
Chars per token Poet data: 3.708941611409137 +- 0.2178069726711261
Chars per token Base data: 3.2744861379271226 +- 0.06663968659758945
All Runs Results: C:\Users\micha\Desktop\Git\Tensorflow-Shorts\PoetGen\utils\tokenizers\Unigram\processed_tokenizer.json # Runs: 100 # Samples: 100
Chars per token Poet data: 3.3113513209024914 +- 0.2575136399035279
Chars per token Base data: 2.7192276740545798 +- 0.052771969738579956
All Runs Results: C:\Users\micha\Desktop\Git\Tensorflow-Shorts\PoetGen\utils\tokenizers\BPE\processed_tokenizer.json # Runs: 100 # Samples: 100
Chars per token Poet data: 3.775447931450171 +- 0.2103308012833001
Chars per token Base data: 3.2021773326053076 +- 0.0727221792330741
All Runs Results: C:\Users\micha\Desktop\Git\Tensorflow-Shorts\PoetGen\utils\tokenizers\BPE\syllabs_processed_tokenizer.json # Runs: 100 # Samples: 100
Chars per token Poet data: 2.440635654391135 +- 0.07655783491847758
Chars per token Base data: 2.1241338036944395 +- 0.026589543206229683
All Runs Results: roberta-base # Runs: 100 # Samples: 100
Chars per token Poet data: 1.495462998759727 +- 0.04666503648004717
Chars per token Base data: 1.7812737039138025 +- 0.03512295831536827
All Runs Results: ufal/robeczech-base # Runs: 100 # Samples: 100
Chars per token Poet data: 2.7246843179053486 +- 0.15425927060750147
Chars per token Base data: 3.5555354739904295 +- 0.07900823431947528
All Runs Results: C:\Users\micha\Desktop\Git\Tensorflow-Shorts\PoetGen\utils\tokenizers\BPE\new_syllabs_processed_tokenizer.json # Runs: 100 # Samples: 100
Chars per token Poet data: 2.4274444117466683 +- 0.08584493273997632
Chars per token Base data: 2.1197043558484014 +- 0.025161001539278798
