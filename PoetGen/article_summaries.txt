Revisiting Neural Language Modelling with Syllable
link: https://arxiv.org/pdf/2010.12881.pdf
    - ! RNN (LSTM) based work

    - Syllables allow open vocab, like word-sub-word-char setup
    - Syllables => Speech units, sub-words => language units
    - Syllables extraction: " “A syl-la-ble con-tains a sin-gle vow-el u-nit” " or Dictionary-based
    - Previous failed to beat char based at word-level (closed-vocab == can't add words)
    - New tests: 1. open-vocab, 2. 20 languages from 6, 3. rule-based and hyphenation tools syllabification
    - Measure perplexity given character-tokenizer
    - Data: WikiText-2-raw
    - For Czech syllables grow more than morphemes using Dictionary = Difficult to determine syllables boundaries
    - Low overlap of morphemes and syllables (under 5 %), compared to Byte Pair Encoding with over (15-75 %)
    - Little more overlap if morphemes are unsupervised
    - Syllables have better (smaller) perplexity

ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models
link: https://arxiv.org/pdf/2212.10474.pdf
    - ! Token-free (tokenized by utf-8 char only)
    - ! decoder-only (GPT style), end-to-end
    - ! only 4-verse (AABB, ABAB, ABBA)
    - ! Large Training Corpus is Important

    - Emphasis on:  - Rhyme schema (ABBA, AABB, ...)
                    - Alliteration = words in verse with same starting character => 3 levels (low, medium, high)
                    - Meter (Czech Metrum) = specific mixture of stressed and unstressed syllables (přízvučné a nepřízvučné slabiky)
    - Before this:  - Human guidance by knowledge injection (Hard-coded constraints, filtering)
                    - Pipeline with: content planing, rhyme pairs generation, sketch-to-poet generation
                    - Leeds to error accumulation
    - Because of poetry reasons (new words creation), tokenizers may struggle to provide good tokens -> token-free (utf-8) allows character expression
    -> Observation: Syllable + Character tokenizer may be good for this
    - Evaluation:   - Rhyme Score = Recall of verses that should rhyme and recall of verses that shouldn't, average of these 2
                    - Alliteration Score = Accuracy of correct alliteration
                    - Meter Score = Accuracy of correct meter
                    - Coherence = Trained BERT for next sentence prediction, alignment Accuracy
    - Results:  - Rhyme:    - ByGPT5 and ByT5 (those with utf-8 tokenizer) better
                - Meter:    - Smaller GPTs better than their medium cunterparts
                            - mT5 better than ByGPT5 => Sub-word tokenizer enough for Meter
                - Alliteration: - Bad for all models
    - Humans are food at rhyming

    - Encoder-Decoder In Code triple (rhyme, scheme, meter) = style
    - Each style = special token
    - Decoder-only = Style by string prompt 
    -> Observation: We already do this.

GPoeT-2: A GPT-2 Based Poem Generator
link: https://arxiv.org/pdf/2205.08847.pdf
    - ! GPT2 based (Like us for now)
    - ! 5-verse with fixed rhyme schema AABBA (Limericks)
    - ! No seeding or posterior constraints

    - Good Poetry:  - Lexical diversity (Type-Token-Ratio = Unique Tokens/Tokens)
                    - Subject continuity (BERT-based Embedding Distance, WordNet-based Similarity Metric, Content Classification)
                    - Syntactically correct
                    - Following rhyme structure
                    => Allows ranking
    - Method:   - 2Stage, Forward and Reverse Modelling (For Reverse => reverse token order on each line)
                - First line by Forward, other 4 lines by Reverse
                - Postprocessing only for Grammar
    
    - Fine-tuning:  Normal GPT => Minimize Negative Log-Likelihood (NLL)

Neural reading: Insights from the analysis of poetry generated by artificial neural networks
link: https://onlinelibrary.wiley.com/doi/10.1111/oli.12274
    - ! RNN Models (2-4 Layers  512-1024 Neurons)
    - ! Models aren't large
    - ! Char-by-Char

    - Before Neural-Nets:   - Rule based generation
                            - Random word swapping, phrases recombination
                            - Seed with nouns and than follow correct grammar
                            - Just to add randomness

    - Neural Reading:   - Train on large corpus (distant reading)
                        - Resemblance => mimicking text in corpus

    - Results:  - Mimics metric scheme and hexametric meter
                - Creates new words (neologisms)
                - Nonsensical
                - One author Corpora:   - Follows the style of the author
                                        - Struggles with narrative fragment
                                        - Mostly good for word play
                - Small corpus => Grammatical inconsistency

Fine-tuning the English GPT-2 in any language with Hugging Face
link: https://github.com/piegu/fastai-projects/blob/master/finetuning-English-GPT2-any-language-Portuguese-HuggingFace-fastaiv2.ipynb

    - Pipeline: - Use translator models between input and output
                - (In Langue - Model Language Translator) => (Pre-trained Model) => (Model Language - Out Language Translator)
                - Risk of loosing target language nuances

    - Fine-tune:    - Model from scratch needs 100s GBs of data
                    - Fine-tune on just 1s Gbs of data
                    - Techniques: Mixed precision, Gradual unfreezing, Differential learning rates, Distributed training
    
    - Tokenizer:    - Byte Level BPE Tokenizer
                    - Import to GPT2 Tokenizer
                    - Update Embedding matrix